# ðŸ‡¯ðŸ‡µ Japanese Tokenization Research

This project compares multiple Japanese tokenization algorithms:  
- **MeCab** (via [Fugashi](https://github.com/polm/fugashi))  
- **SudachiPy**  
- **SentencePiece** (using a pretrained model)  
- **Juman++** (optional)

The script runs all available tokenizers on given Japanese texts and exports a comparison CSV with token counts.

---

## ðŸ“¦ Requirements

Install Python dependencies:

```bash
pip install pandas fugashi sudachipy sentencepiece

**Project Structure**
tokenization-research/
â”‚
â”œâ”€â”€ tokenization_compare.py    # Main script
â”œâ”€â”€ spm.model                  # Pretrained SentencePiece model (downloaded)
â”œâ”€â”€ japanese_tokenization_results.csv  # Output file
â””â”€â”€ README.md
